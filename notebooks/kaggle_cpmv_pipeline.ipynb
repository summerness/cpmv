{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Copy-Move Segmentation Pipeline\n",
        "\n",
        "This notebook walks through the entire workflow end-to-end:\n",
        "1. Prepare CSV metadata from the folder layout.\n",
        "2. Train the five required models (ConvNeXt/UNet++, Swin-DeepLab, CMSeg-Lite variants).\n",
        "3. Optionally run single-model inference to inspect outputs.\n",
        "4. Blend the five checkpoints for the final ensemble submission.\n",
        "\n",
        "> **Note**: When running on Kaggle, the notebook auto-detects datasets mounted under `/kaggle/input` that contain a `train_images` folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Environment Setup\n",
        "Install dependencies once per environment." 
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0.1 Notebook Imports\n",
        "Add the repository to `sys.path` so we can import modules without shell commands." 
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "project_root = Path().resolve()\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.append(str(project_root))\n",
        "print(f'Project root: {project_root}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Preparation\n",
        "Scan the dataset folders (auto-detected from Kaggle `input/` if available) and generate CSV metadata for training/inference." 
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from prepare_data import build_train_df, build_test_csv\n",
        "\n",
        "input_root = Path('/kaggle/input')\n",
        "detected_roots = []\n",
        "if input_root.exists():\n",
        "    for candidate in input_root.iterdir():\n",
        "        if candidate.is_dir() and (candidate / 'train_images').exists():\n",
        "            detected_roots.append(candidate)\n",
        "\n",
        "if detected_roots:\n",
        "    data_root = detected_roots[0]\n",
        "else:\n",
        "    data_root = Path('data')\n",
        "print(f'Detected data root: {data_root}')\n",
        "\n",
        "train_images = data_root / 'train_images'\n",
        "train_masks = data_root / 'train_masks'\n",
        "supp_images = data_root / 'supplemental_images'\n",
        "supp_masks = data_root / 'supplemental_masks'\n",
        "test_images = data_root / 'test_images'\n",
        "sample_sub = data_root / 'sample_submission.csv'\n",
        "\n",
        "output_data_dir = Path('data')\n",
        "output_data_dir.mkdir(parents=True, exist_ok=True)\n",
        "train_csv = output_data_dir / 'train.csv'\n",
        "test_csv = output_data_dir / 'test.csv'\n",
        "\n",
        "if not train_images.exists() or not train_masks.exists():\n",
        "    raise FileNotFoundError('train_images/train_masks directories not found. Update data_root or mount dataset properly.')\n",
        "\n",
        "print('Building primary train dataframe...')\n",
        "primary_df = build_train_df(train_images, train_masks, source_label='primary', allow_missing_mask=False)\n",
        "frames = [primary_df]\n",
        "if supp_images.exists() and supp_masks.exists():\n",
        "    print('Including supplemental data...')\n",
        "    supp_df = build_train_df(supp_images, supp_masks, source_label='supplemental', allow_missing_mask=False)\n",
        "    frames.append(supp_df)\n",
        "train_df = pd.concat(frames, ignore_index=True)\n",
        "train_df.to_csv(train_csv, index=False)\n",
        "print(f'Saved {len(train_df)} training rows to {train_csv}')\n",
        "\n",
        "if test_images.exists():\n",
        "    build_test_csv(test_images, test_csv, sample_sub if sample_sub.exists() else None)\n",
        "    print(f'Saved test metadata to {test_csv}')\n",
        "else:\n",
        "    print('Test images folder not found; skipping test CSV generation.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train the Five Models\n",
        "Loop over the five configuration files and run the training routine directly within the notebook." 
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import src.train as train_module\n",
        "\n",
        "def run_training(config_path: str):\n",
        "    backup = sys.argv\n",
        "    sys.argv = ['train.py', '--config', config_path]\n",
        "    try:\n",
        "        train_module.main()\n",
        "    finally:\n",
        "        sys.argv = backup\n",
        "\n",
        "training_configs = [\n",
        "    'config/convnext_unetpp_512_base.yaml',\n",
        "    'config/swin_deeplab_512_base.yaml',\n",
        "    'config/cmseg_lite_512_base.yaml',\n",
        "    'config/convnext_unetpp_768_heavyaug.yaml',\n",
        "    'config/cmseg_lite_512_synCM.yaml',\n",
        "]\n",
        "\n",
        "for cfg in training_configs:\n",
        "    print(f'\\n=== Training with {cfg} ===')\n",
        "    run_training(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Single-Model Inference (Optional)\n",
        "Run each trained checkpoint individually, exporting per-model predictions and probability maps." 
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import src.infer as infer_module\n",
        "\n",
        "def run_inference(image_dir: str, checkpoint: str, output_csv: str, prob_dir: str):\n",
        "    backup = sys.argv\n",
        "    sys.argv = [\n",
        "        'infer.py',\n",
        "        '--image-dir', image_dir,\n",
        "        '--checkpoint', checkpoint,\n",
        "        '--output', output_csv,\n",
        "        '--save-prob',\n",
        "        '--prob-dir', prob_dir,\n",
        "    ]\n",
        "    try:\n",
        "        infer_module.main()\n",
        "    finally:\n",
        "        sys.argv = backup\n",
        "\n",
        "image_dir = str(test_images if test_images.exists() else Path('data/test_images'))\n",
        "inference_jobs = [\n",
        "    ('outputs/convnext_unetpp_512_base/best.ckpt', 'predictions_convnext_base.csv', 'probs_convnext_base'),\n",
        "    ('outputs/swin_deeplab_512_base/best.ckpt', 'predictions_swin_base.csv', 'probs_swin_base'),\n",
        "    ('outputs/cmseg_lite_512_base/best.ckpt', 'predictions_cmseg_base.csv', 'probs_cmseg_base'),\n",
        "    ('outputs/convnext_unetpp_768_heavyaug/best.ckpt', 'predictions_convnext_768.csv', 'probs_convnext_768'),\n",
        "    ('outputs/cmseg_lite_512_synCM/best.ckpt', 'predictions_cmseg_syn.csv', 'probs_cmseg_syn'),\n",
        "]\n",
        "\n",
        "for ckpt, out_csv, prob_dir in inference_jobs:\n",
        "    print(f'Running inference for {ckpt}')\n",
        "    run_inference(image_dir, ckpt, out_csv, prob_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Ensemble Submission\n",
        "Average the five probability maps on-the-fly and export the final Kaggle submission." 
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import src.ensemble as ensemble_module\n",
        "\n",
        "def run_ensemble(image_dir: str, checkpoints, output_csv: str, prob_dir: str | None = None):\n",
        "    backup = sys.argv\n",
        "    argv = [\n",
        "        'ensemble.py',\n",
        "        '--image-dir', image_dir,\n",
        "        '--checkpoints',\n",
        "    ] + list(checkpoints) + ['--output', output_csv]\n",
        "    if prob_dir:\n",
        "        argv += ['--save-prob-dir', prob_dir]\n",
        "    sys.argv = argv\n",
        "    try:\n",
        "        ensemble_module.main()\n",
        "    finally:\n",
        "        sys.argv = backup\n",
        "\n",
        "ensemble_checkpoints = [\n",
        "    'outputs/convnext_unetpp_512_base/best.ckpt',\n",
        "    'outputs/swin_deeplab_512_base/best.ckpt',\n",
        "    'outputs/cmseg_lite_512_base/best.ckpt',\n",
        "    'outputs/convnext_unetpp_768_heavyaug/best.ckpt',\n",
        "    'outputs/cmseg_lite_512_synCM/best.ckpt',\n",
        "]\n",
        "ensemble_image_dir = str(test_images if test_images.exists() else Path('data/test_images'))\n",
        "run_ensemble(ensemble_image_dir, ensemble_checkpoints, output_csv='submission.csv', prob_dir='ensemble_probs')\n",
        "print('Ensemble submission saved to submission.csv')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
